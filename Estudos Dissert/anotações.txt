Anotações Redes Neurais Recorrentes

Livro: Recurrent Neural Networks: Design and Applications
Autores: (editores) L. R. Medsker e L. C. Jain
Ano: ---

Capítulo 1 

I - Visão Geral 

Redes recorrentes são utilizadas para aprender padrões sequenciais ou 
que variam com o tempo, caracterizadas pela retorno do sinal em suas conexões.

Exemplos: BAM, Hopfield, Máquina de Boltzmann, redes de backpropagation 
    recorrentes (Hecth-Nielsen, 1990)

Redes Neurais parcialmente simples foram introduzidas por Rummelhart (1986), 
    mútliplas aplicações (predição financeira (Giles, Lawrence, Tsoi) 1997)

I-A - Arquitetura das Redes Recorrentes 

Arquiteturas variam entre completamente interconectadas até parcialmente 
interconectadas.
    --> Nas redes completamente interconectadas, não existe distinção
        entre camadas, e também é possível feedback do próprio neurônio 

    --> Parcialmente conectadas: usadas para aprender cadeias de caracteres, 
        nos quais alguns neurônios pode fazer parte de uma estrutura feedforward
        
    --> Os neurônios que são conectados dão o "contexto sequencial" ao receber
        feedback de outros neurônios, os pesos dessas unidades são processados
        assim como os do neuronio de entrada, usando, por exemplo, 
        backpropagation.

As unidades de contexto (neurônios que recebem o feedback de outros neurônios),
recebem o sinal de feedback atrasado do neuronio original (na Figura 2 são os 
neuronios da segunda camada).

!! --> Os conjuntos de treinamento são as entradas e seus SUCESSORES desejados.







